import json
import kfp
import os
import tarfile

from datetime import datetime
from minio import Minio
from minio.error import (ResponseError,
                         BucketAlreadyOwnedByYou,
                         BucketAlreadyExists)
from notebook.base.handlers import IPythonHandler


class SchedulerHandler(IPythonHandler):

    """REST-ish method calls to execute pipelines as batch jobs"""
    def get(self):
        msg_json = dict(title="Operation not supported.")
        self.write(msg_json)
        self.flush()

    def post(self, *args, **kwargs):
        print('>>> SchedulerHandler.post')

        """Upload endpoint"""
        url = 'http://weakish1.fyre.ibm.com:32488/pipeline'
        endpoint = 'http://weakish1.fyre.ibm.com:30427'
        minio_username = 'minio'
        minio_password = 'minio123'
        bucket_name = 'lresende'

        options = self.get_json_body()

        print('>>>')
        print(options)

        # Iterate through the components and create a list of input components
        links = {}
        labels = {}
        for component in options['pipeline_data']['nodes']:
            # Set up dictionary to track node id's of inputs
            links[component['id']] = []
            if 'links' in component['inputs'][0]:
                for link in component['inputs'][0]['links']:
                    links[component['id']].append(link['node_id_ref'])

            # Set up dictionary to link component id's to
            # component names (which are ipynb filenames)

            # Component id's are generated by CommonCanvas
            labels[component['id']] = component['app_data']['ui_data']['label']

        # Initialize minioClient with an endpoint and access/secret keys.
        minio_client = Minio('weakish1.fyre.ibm.com:30427',
                             access_key=minio_username,
                             secret_key=minio_password,
                             secure=False)

        # Make a bucket with the make_bucket API call.
        try:
            if not minio_client.bucket_exists(bucket_name):
                minio_client.make_bucket(bucket_name)
        except BucketAlreadyOwnedByYou as err:
            pass
        except BucketAlreadyExists as err:
            pass
        except ResponseError as err:
            raise

        def cc_pipeline():
            # Create dictionary that maps component Id to its ContainerOp instance
            notebookops = {}
            # Create component for each node from CommonCanvas
            for componentId, inputs in links.items():
                notebookPath = labels[componentId]
                name = os.path.basename(notebookPath).split(".")[0]
                output_filename = options['pipeline_name'] + datetime.now().strftime("%m%d%H%M%S") + ".tar.gz"
                extracted_dir_from_tar = "jupyter-work-dir"

                print('>>> componentId {}'.format(componentId))
                print('>>> inputs {}'.format(inputs))
                print('>>> name {}'.format(name))
                print('>>> output_filename {}'.format(output_filename))
                print('>>> extracted_dir_from_tar {}'.format(extracted_dir_from_tar))



                notebookops[componentId] = \
                    kfp.dsl.ContainerOp(name=name,
                                        image=options['docker_image'],
                                        command=['sh', '-c'],
                                        arguments=['pip install papermill && '
                                                   'apt install -y wget &&'
                                                   'wget https://dl.min.io/client/mc/release/linux-amd64/mc && '
                                                   'chmod +x mc && '
                                                   './mc config host add aiworkspace '+endpoint+' '+minio_username+' '+minio_password+' && '
                                                   './mc cp aiworkspace/'+bucket_name+'/'+output_filename+ ' . && '
                                                   'mkdir -p '+extracted_dir_from_tar+' && '
                                                   'cd '+extracted_dir_from_tar+' && '
                                                   'tar -zxvf ../'+output_filename+' --strip 1 && '
                                                   'echo $(pwd) && '
                                                   'ls -la && '
                                                   'papermill '+name+'.ipynb '+name+'_output.ipynb && '
                                                   '../mc cp '+name+'_output.ipynb aiworkspace/'+bucket_name+'/'+name+'_output.ipynb'
                                        ])

                print('>>> after notebookOps created')
                try:
                    full_notebook_path = os.path.join(os.getcwd(), notebookPath)
                    notebook_work_dir = os.path.dirname(full_notebook_path)

                    print('>>> will create {} with contents from {}'.format(output_filename, notebook_work_dir))

                    with tarfile.open(output_filename, "w:gz") as tar:
                        tar.add(notebook_work_dir, arcname=output_filename)

                    print('>>> tar file created')
                    minio_client.fput_object(bucket_name=bucket_name,
                                             object_name=output_filename,
                                             file_path=output_filename)

                    print('>>> tar file uploaded to minio')
                except ResponseError as re:
                    print(re)

            # Add order based on list of inputs for each component.
            for componentId, inputs in links.items():
                for inputComponentId in inputs:
                    notebookops[componentId].after(notebookops[inputComponentId])

            print('>>> pipeline dependencies set')

        pipeline_name = options['pipeline_name']+datetime.now().strftime("%m%d%H%M%S")

        print('>>> pipeline_name {}'.format(pipeline_name))

        if not os.path.exists('pipeline_files'):
            os.mkdir('pipeline_files')

        pipeline_path = 'pipeline_files/'+pipeline_name+'.tar.gz'

        # Compile the new pipeline
        kfp.compiler.Compiler().compile(cc_pipeline,pipeline_path)

        print('>>> kfp compiled')

        # Upload the compiled pipeline and create an experiment and run
        client = kfp.Client(host=url)
        kfp_pipeline = client.upload_pipeline(pipeline_path, pipeline_name)

        print('>>> kfp uploaded')

        client.run_pipeline(experiment_id=client.create_experiment(pipeline_name).id,
                            job_name=datetime.now().strftime("%m%d%H%M%S"),
                            pipeline_id=kfp_pipeline.id)

        print('>>> kfp run')

    def send_message(self, message):
        self.write(message)
        self.flush()

    def send_success_message(self, message, job_url):
        self.set_status(200)
        msg = json.dumps({"status": "ok",
                          "message": message,
                          "url": job_url})
        self.send_message(msg)

    def send_error_message(self, status_code, error_message):
        self.set_status(status_code)
        msg = json.dumps({"status": "error",
                          "message": error_message})
        self.send_message(msg)

