import json
import kfp
import os
import re
import tarfile
import tempfile

from datetime import datetime
from notebook.base.handlers import IPythonHandler
from notebook.pipeline import NotebookOp
from ai_workspace.metadata import Metadata, MetadataManager, FileMetadataStore
from ai_workspace.pipeline import Pipeline, Operation, PipelineParser
from minio import Minio
from minio.error import ResponseError, BucketAlreadyOwnedByYou, BucketAlreadyExists


class SchedulerHandler(IPythonHandler):
    metadata_manager = MetadataManager(namespace="runtime",
                                       store=FileMetadataStore(namespace='runtime'))

    """REST-ish method calls to execute pipelines as batch jobs"""
    def get(self):
        msg_json = dict(title="Operation not supported.")
        self.write(msg_json)
        self.flush()

    def post(self, *args, **kwargs):
        self.log.debug("Pipeline SchedulerHandler now executing post request")

        """Upload endpoint"""
        runtime_configuration = self.metadata_manager.get('kfp')

        if not runtime_configuration:
            raise RuntimeError("Runtime metadata not available.")

        api_endpoint = runtime_configuration.metadata['api_endpoint']
        cos_endpoint = runtime_configuration.metadata['cos_endpoint']
        cos_username = runtime_configuration.metadata['cos_username']
        cos_password = runtime_configuration.metadata['cos_password']
        bucket_name = runtime_configuration.metadata['cos_bucket']

        self.log.info('Runtime configuration: \n {} \n {} \n {} \n {} \n {}'
                      .format(api_endpoint, cos_endpoint, cos_username, cos_password, bucket_name))

        options = self.get_json_body()

        self.log.debug("JSON options: %s", options)

        # Iterate through the components and create a list of input components
        links = {}
        labels = {}
        docker_images = {}
        for component in options['pipeline_data']['nodes']:
            # Set up dictionary to track node id's of inputs
            links[component['id']] = []
            if 'inputs' in component.keys():
                if 'links' in component['inputs'][0]:
                    for link in component['inputs'][0]['links']:
                        links[component['id']].append(link['node_id_ref'])

            # Set up dictionary to link component id's to
            # component names (which are ipynb filenames)

            # Component id's are generated by CommonCanvas
            labels[component['id']] = component['app_data']['notebook']
            docker_images[component['id']] = component['app_data']['docker_image']

        # Initialize minioClient with an endpoint and access/secret keys.
        minio_client = Minio(endpoint=re.sub(r'^https?://', '',cos_endpoint),
                             access_key=cos_username,
                             secret_key=cos_password,
                             secure=False)

        # Make a bucket with the make_bucket API call.
        try:
            if not minio_client.bucket_exists(bucket_name):
                minio_client.make_bucket(bucket_name)
        except BucketAlreadyOwnedByYou:
            self.log.warning("Minio bucket already owned by you", exc_info=True)
            pass
        except BucketAlreadyExists:
            self.log.warning("Minio bucket already exists", exc_info=True)
            pass
        except ResponseError:
            self.log.error("Minio error", exc_info=True)
            raise

        def cc_pipeline():
            # Create dictionary that maps component Id to its ContainerOp instance
            notebookops = {}
            # Create component for each node from CommonCanvas
            for componentId, inputs in links.items():
                notebookPath = labels[componentId]
                name = os.path.basename(notebookPath).split(".")[0]
                output_filename = options['pipeline_name'] + datetime.now().strftime("%m%d%H%M%S") + ".tar.gz"
                docker_image = docker_images[componentId]

                self.log.debug("Creating pipeline component :\n "
                               "componentID : %s \n "
                               "inputs : %s \n "
                               "name : %s \n "
                               "output_filename : %s \n "
                               "docker image : %s \n ",
                               componentId,
                               inputs,
                               name,
                               output_filename,
                               docker_image)

                notebookops[componentId] = NotebookOp(name=name,
                                                      notebook=str(name),
                                                      cos_endpoint=cos_endpoint,
                                                      cos_user=cos_username,
                                                      cos_password=cos_password,
                                                      cos_bucket=bucket_name,
                                                      cos_pull_archive=output_filename,
                                                      image='tensorflow/tensorflow:1.13.2-gpu-py3-jupyter')

                self.log.info("NotebookOp Created for Component %s", componentId)

                try:
                    full_notebook_path = os.path.join(os.getcwd(), notebookPath)
                    notebook_work_dir = os.path.dirname(full_notebook_path)

                    self.log.debug("Creating TAR archive %s with contents from %s", output_filename, notebook_work_dir)

                    with tempfile.TemporaryDirectory() as archive_temp_dir:
                        with tarfile.open(archive_temp_dir+output_filename, "w:gz") as tar:
                            tar.add(notebook_work_dir, arcname="")
                        self.log.debug("Creating temp directory for archive TAR : %s", archive_temp_dir)
                        self.log.info("TAR archive %s created", output_filename)

                        minio_client.fput_object(bucket_name=bucket_name,
                                                 object_name=output_filename,

                                                 file_path=archive_temp_dir+output_filename)

                        self.log.debug("TAR archive %s pushed to bucket : %s ", output_filename, bucket_name)

                except ResponseError:
                    self.log.error("ERROR : From object storage", exc_info=True)

            # Add order based on list of inputs for each component.
            if links:
                for componentId, inputs in links.items():
                    for inputComponentId in inputs:
                        notebookops[componentId].after(notebookops[inputComponentId])

            self.log.info("Pipeline dependencies are set")

        pipeline_name = options['pipeline_name']+datetime.now().strftime("%m%d%H%M%S")

        with tempfile.TemporaryDirectory() as temp_dir:
            pipeline_path = temp_dir+'/'+pipeline_name+'.tar.gz'

            self.log.info("Pipeline : %s", pipeline_name)
            self.log.debug("Creating temp directory %s", temp_dir)

            # Compile the new pipeline
            kfp.compiler.Compiler().compile(cc_pipeline,pipeline_path)

            self.log.info("Kubeflow Pipeline successfully compiled!")
            self.log.debug("Kubeflow Pipeline compiled pipeline placed into %s", pipeline_path)

            # Upload the compiled pipeline and create an experiment and run
            client = kfp.Client(host=api_endpoint)
            kfp_pipeline = client.upload_pipeline(pipeline_path, pipeline_name)

            self.log.info("Kubeflow Pipeline successfully uploaded to : %s", api_endpoint)

        client.run_pipeline(experiment_id=client.create_experiment(pipeline_name).id,
                            job_name=datetime.now().strftime("%m%d%H%M%S"),
                            pipeline_id=kfp_pipeline.id)

        self.log.info("Starting Kubeflow Pipeline Run...")

    def send_message(self, message):
        self.write(message)
        self.flush()

    def send_success_message(self, message, job_url):
        self.set_status(200)
        msg = json.dumps({"status": "ok",
                          "message": message,
                          "url": job_url})
        self.send_message(msg)

    def send_error_message(self, status_code, error_message):
        self.set_status(status_code)
        msg = json.dumps({"status": "error",
                          "message": error_message})
        self.send_message(msg)


